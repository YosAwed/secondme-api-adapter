# Second MeでOllamaを使用するためのセットアップガイド

## 前提条件

- Ollamaがインストールされており、ローカルで実行されていること
- Llama 3.2モデルがOllamaにダウンロードされていること
- Pythonがインストールされていること
- Second Meがセットアップされていること

## 1. APIアダプターのセットアップ

### アダプタースクリプトのインストール

1. 提供したコード（`ollama-adapter.py`）を保存します
2. 必要なライブラリをインストールします:

```bash
pip install flask requests
```

3. アダプタースクリプトを実行します:

```bash
python ollama-adapter.py
```

これにより、`http://localhost:8000`でOpenAI互換のAPIエンドポイントが利用可能になります。

## 2. Second Meの設定変更

Second Meの設定ファイルまたは環境変数を編集して、OpenAIのエンドポイントを以下のように変更します:

```
OPENAI_API_BASE=http://localhost:8080
OPENAI_API_KEY=dummy  # 任意の値で構いません
```

注: Second Meは8000番ポートを使用しているため、アダプターは8080番ポートで実行します。

## 3. モデル名の設定

アダプタースクリプト内のモデル名変数を必要に応じて変更できます。現在の設定は以下の通りです：

```python
CHAT_MODEL_NAME = "llama3.2"      # チャット用モデル
EMBEDDING_MODEL_NAME = "nomic-embed-text"  # 埋め込み用モデル
```

## 4. トラブルシューティング

### リクエスト/レスポンス形式の問題

APIの応答形式に問題がある場合は、アダプタースクリプトの`chat_completions`または`embeddings`関数内の変換ロジックを確認・修正します。

### エラーログの確認

問題が発生した場合は、アダプタースクリプトのコンソール出力とSecond Meのログを確認してください。

問題の特定には以下の方法も効果的です:

1. curlを使用して直接APIをテストする
2. デバッグログを有効にする
3. リクエスト/レスポンスの内容をログに記録する

### litellmとの違い

litellmを使用した場合と異なり、このアダプターは特定のOllamaモデル用にカスタマイズされています。litellmで問題があった場合、このアプローチの方が制御しやすい可能性があります。

## 5. 高度な設定

必要に応じて、以下の拡張機能を実装できます:

- ストリーミングレスポンスのサポート
- 複数モデルのサポート
- トークン数のより正確な計算
- エラーハンドリングの改善